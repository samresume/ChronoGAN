{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DualGAN: Dual Adversarial Time Series Generation via GANs and Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# 1. DualGAN model\n",
    "from dualgan import dualgan\n",
    "\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = []\n",
    "generated_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "## Data loading\n",
    "data_name = 'stock'\n",
    "seq_len = 24\n",
    "\n",
    "if data_name in ['stock', 'electricity', 'ECG']:\n",
    "  original_data.append(real_data_loading(data_name, seq_len))\n",
    "\n",
    "print(data_name + ' dataset is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sine dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "## Data loading\n",
    "data_name = 'sine'\n",
    "seq_len = 64\n",
    "\n",
    "if data_name == 'sine':\n",
    "  # Set number of samples and its dimensions\n",
    "  no, dim = 10000, 4\n",
    "  original_data.append(sine_data_generation(no, seq_len, dim))\n",
    "    \n",
    "print(data_name + ' dataset is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "## Data loading\n",
    "data_name = 'ECG'\n",
    "seq_len = 140\n",
    "\n",
    "if data_name in ['stock', 'electricity', 'ECG']:\n",
    "  original_data.append(real_data_loading(data_name, seq_len))\n",
    "   \n",
    "print(data_name + ' dataset is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWANSF dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "## Data loading\n",
    "data_name = 'SWANSF'\n",
    "seq_len = 60\n",
    "\n",
    "if data_name in ['SWANSF','stock', 'energy', 'ECG']:\n",
    "  original_data.append(real_data_loading(data_name, seq_len))\n",
    "   \n",
    "print(data_name + ' dataset is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network parameters\n",
    "\n",
    "DualGAN network parameters should be optimized for different datasets.\n",
    "\n",
    "- hidden_dim: hidden dimensions -> input 'same' or a number like 8\n",
    "- num_layer: number of layers\n",
    "- iteration: number of training iterations\n",
    "- batch_size: the number of samples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newtork parameters\n",
    "parameters = dict()\n",
    "\n",
    "parameters['hidden_dim'] = 'same'\n",
    "parameters['iterations'] = 7 * 1000\n",
    "parameters['batch_size'] = 128\n",
    "parameters['num_layer'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run synthetic time-series data generation\n",
    "\n",
    "DualGAN uses the original data and network parameters to return the generated synthetic data.\n",
    "\n",
    "It also utilizes the number of samples that need to be generated. If you enter 'same', it will produce an equal number of synthetic samples to match the real samples that you have. Otherwise, please enter a specific number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Embedding Network Training\n",
      "step: 0/7000, AE_loss: 3.3492, AE_D_loss: 1.3232\n"
     ]
    }
   ],
   "source": [
    "dualgan_result = dualgan(original_data[0], parameters, 'same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n",
    "### 1. Discriminative score\n",
    "\n",
    "To evaluate the classification accuracy between original and synthetic data using post-hoc RNN network. The output is |classification accuracy - 0.5|.\n",
    "\n",
    "- metric_iteration: the number of iterations for metric computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_iteration = 6\n",
    "\n",
    "discriminative_score = list()\n",
    "for _ in range(metric_iteration):\n",
    "  temp_disc = discriminative_score_metrics(original_data[0], dualgan_result)\n",
    "  discriminative_score.append(temp_disc)\n",
    "\n",
    "print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n",
    "### 2. Predictive score\n",
    "\n",
    "To evaluate the prediction performance on train on synthetic, test on real setting. More specifically, we use Post-hoc RNN architecture to predict one-step ahead and report the performance in terms of MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_score = list()\n",
    "for tt in range(metric_iteration):\n",
    "  temp_pred = predictive_score_metrics(original_data[0], dualgan_result)\n",
    "  predictive_score.append(temp_pred)   \n",
    "    \n",
    "print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n",
    "### 3. Visualization\n",
    "\n",
    "We visualize the original and synthetic data distributions using PCA and tSNE analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(original_data[0], dualgan_result, 'pca', 'Sines')\n",
    "visualization(original_data[0], dualgan_result, 'tsne', 'Sines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
